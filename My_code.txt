''' class DecisionTree:
    '''
    this is a decision tree classifier.
    '''
    def __init__(self, min_samples=2, max_depth=3):
        self.min_samples = min_samples
        self.max_depth = max_depth

    def split_data(self, dataset, feature, threshold):
        """
        Splits the dataset into two subsets based on a feature and its threshold.
        
        Parameters:
        - dataset: The dataset to split.
        - feature: The index of the feature to split on.
        - threshold: The value to compare against.
        
        Returns:
        - left: Subset where the feature value is less than or equal to the threshold.
        - right: Subset where the feature value is greater than the threshold.
        """
        left_dataset = []
        right_dataset = []

        # loop through each row in the dataset and split into left/right
        for row in dataset:
            if row[feature] <= threshold:
                left_dataset.append(row)
            else:
                right_dataset.append(row)

        return left_dataset, right_dataset
    #convert the left and right datasets into numpy arrays
        left_dataset = np.array(left_dataset)
        right_dataset = np.array(right_dataset)
        
        return left_dataset, right_dataset
    # this function calculates the Gini impurity for a given dataset
    # Gini impurity is a measure of how often a randomly chosen element from the set
    # would be incorrectly labeled if it was randomly labeled according to the distribution of labels in the dataset.
    # It is calculated as:
    # Gini = 1 - sum(p_i^2) for all classes i
    # where p_i is the proportion of class i in the dataset.
    # The lower the Gini impurity, the better the split.

def calculate_gini(self, y):
        
    '''  Computes the entropy for given labels
    Returns: float: Entropy value'''
       
    entropy = 0.0
    # this initializes the entropy to zero
    # use numpy's unique function to get the unique labels and their counts
    labels = np.unique(y)
    for label in labels:
        # find examples in y that have the current label
        label_examples = y[y == label]
        # calculate the probability of the current label
        p1 = len(label_examples) / len(y)
        entropy += -p1 * np.log2(p1)
                
    return entropy  # return the entropy value

# Function to calculate Information Gain using Gini/Entropy
def calculate_gini(self, parent, left, right):
    '''
    Computes the Information Gain from splitting the parent dataset into two subsets.

    Parameters:
    - parent (ndarray): The parent dataset before the split.
    - left (ndarray): The left subset of the parent dataset after the split.
    - right (ndarray): The right subset of the parent dataset after the split.

    Returns:
    - float: Information Gain from the split.
    '''
    
    # initialize the information gain to zero
    information_gain = 0.0
    
    # compute the entropy of the parent dataset
    parent_entropy = self.entropy(parent)
    
    # calculate the weights for left and right datasets/nodes
    weight_left = len(left) / len(parent)
    weight_right = len(right) / len(parent)
    
    # compute the entropy of the left and right datasets/nodes
    entropy_left, entropy_right = self.entropy(left), self.entropy(right)
    
    # calculate the weighted entropy
    weighted_entropy = (weight_left * entropy_left) + (weight_right * entropy_right)

    information_gain = parent_entropy - weighted_entropy
    return information_gain  # return the information gain value
#10101010101010-------------------------------------------------------------------------------------------------------------------------------------------------
# Function to find the best split for a dataset
def find_best_split(self, dataset):
    """
    Finds the best feature and threshold to split the dataset.

    Parameters:
    - dataset: The dataset to find the best split for.

    Returns:
    - Node: A Node containing the best feature, threshold, and gain.
    """
    
    # initialize variables to store the best split
    best_gain = 0.0
    best_feature = None
    best_threshold = None
    
    # loop through each feature in the dataset
    for feature in range(dataset.shape[1] - 1):  # exclude the last column (target variable)
        # get unique values of the feature
        thresholds = np.unique(dataset[:, feature])
        
        # loop through each threshold value
        for threshold in thresholds:
            # split the dataset into left and right subsets
            left, right = self.split_data(dataset, feature, threshold)
            
            # check if either subset is empty
            if len(left) == 0 or len(right) == 0:
                continue
            
            # calculate information gain from this split
            gain = self.calculate_gini(dataset[:, -1], left[:, -1], right[:, -1])
            
            # update the best split if this gain is better than previous ones
            if gain > best_gain:
                best_gain = gain
                best_feature = feature
                best_threshold = threshold
                
    return Node(feature=best_feature, threshold=best_threshold, gain=best_gain)





''''




